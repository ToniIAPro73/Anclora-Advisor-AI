# An√°lisis y Plan Integral de Mejoras: Anclora Advisor AI

Tras analizar tu documento [Plan_de_Mejoras_RAG.md](file:///c:/Users/Usuario/Workspace/01_Proyectos/anclora-advisor-ai/docs/Plan_de_Mejoras_RAG.md) y tomando en consideraci√≥n las altas capacidades de tu hardware (LG gram Pro equipado con **Intel Core Ultra 7 con NPU**, 32GB LPDDR5x y **NVIDIA RTX 3050**), presento mi revisi√≥n profesional orientada a maximizar el rendimiento mediante tecnolog√≠as Open Source.

---

## 1. An√°lisis del Documento Actual ([Plan_de_Mejoras_RAG.md](file:///c:/Users/Usuario/Workspace/01_Proyectos/anclora-advisor-ai/docs/Plan_de_Mejoras_RAG.md))

### üõ†Ô∏è Qu√© Mejorar√≠a
**Distribuci√≥n de VRAM/RAM y Cuantizaci√≥n (Punto 1.2 y 1.3):**
*   **Motivaci√≥n:** El documento asume cargar partes cr√≠ticas en la RTX 3050 y el resto en RAM. La RTX 3050 de port√°til suele tener 4GB de VRAM. Un modelo Qwen 2.5 14B en `Q4_K_M` pesa unos ~8.5GB. Intentar usar un `Q6_K` (que roza los 11.5GB) ralentizar√° asim√©tricamente la generaci√≥n debido a los constantes saltos entre la VRAM y la RAM del sistema.
*   **Mejora:** Establecer estrictamente la cuantizaci√≥n **`Q4_K_M` para modelos de 14B** y delegar en Ollama la gesti√≥n de memoria. Ollama detectar√° la VRAM e inherentemente saturar√° la NVIDIA `cuda`, dejando el desbordamiento a los r√°pidos 32GB de LPDDR5x. Es el balance perfecto entre precisi√≥n sem√°ntica-legal y rendimiento.

### ‚ùå Qu√© Quitar√≠a
**Sustituci√≥n de Ollama por ipex-llm como motor principal (Punto 1.1):**
*   **Motivaci√≥n:** `ipex-llm` (Intel Extension for PyTorch) es brillante para sistemas *100% Intel* (iGPU/NPU). Sin embargo, la arquitectura de tu LG Gram cuenta con una potente **GPU dedicada de NVIDIA (RTX 3050)**. Las arquitecturas basadas en CUDA son el est√°ndar insuperable en inferencia r√°pida con Ollama/llama.cpp.
*   **Decisi√≥n:** Eliminar√≠a la recomendaci√≥n de sustituir el motor LLM principal por `ipex-llm`. Es vital **mantener Ollama con el framework NVIDIA habilitado** para el procesamiento pesado de texto, preservando as√≠ la potencia geom√©trica y de tensor cores de la RTX 3050.

### ‚ûï Qu√© A√±adir√≠a
1.  **Semantic Chunking (Fragmentaci√≥n Sem√°ntica):**
    *   **Motivaci√≥n:** El documento propone un chunking "Contextual" por estructura (art√≠culos/t√≠tulos). A√±adir√≠a dividir basado en significado. Las leyes a veces mezclan varios supuestos t√©cnicos largos en un mismo p√°rrafo.
    *   **Adici√≥n:** Cortar din√°micamente cuando el significado sem√°ntico cambia, lo cual eleva much√≠simo el recall en la recuperaci√≥n vectorial en casos pr√°cticos ("¬øqu√© hago si...?").
2.  **Reranker y Embeddings nativos exclusivamente en el NPU:**
    *   **Motivaci√≥n:** Al dejar la NVIDIA para el LLM, necesitamos hardware sin cuellos de botella para el buscador neuronal. Aqu√≠ es donde brilla tu NPU de Intel Core Ultra.
    *   **Adici√≥n:** Implementar de forma expl√≠cita modelos peque√±os (como `bge-m3` o `jina-reranker-multilingual`) compilados en **OpenVINO u Optimum-Intel**. Al derivar esto al NPU, liberas el sistema base, reduces consumo t√©rmico e incrementas fluidez, construyendo un verdadero pipeline de IA heterog√©nea.
3.  **Framework de Monitoreo RAG (RAGAS / TruLens):**
    *   **Motivaci√≥n:** Al ser un asistente legal/fiscal, las alucinaciones pueden ser letales. El plan asume mejora al cambiar el modelo pero no explica c√≥mo se medir√°.
    *   **Adici√≥n:** A√±adir un sistema anal√≠tico de m√©tricas Open Source para evaluar sistem√°ticamente la Recisi√≥n del Contexto, la Fidelidad y la Relevancia de la respuesta automatizada antes de pasarlo a producci√≥n con usuarios reales.

---

## 2. Plan Integral de Mejoras de Aplicaci√≥n (Anclora Advisor AI)

A continuaci√≥n, un plan de acci√≥n para el sistema completo, agrupado por orden de prioridad para construir un producto s√≥lido, escalable y eficiente *self-hosted*.

### üî¥ Prioridad 1: Core de Inteligencia Artificial (El Motor Sem√°ntico y T√©cnico)
*Asegurar que las respuestas y la l√≥gica t√©cnica fluyan exactas, apoyadas en arquitectura pesada.*

1.  **Arquitectura Heterog√©nea de Hardware a Nivel M√°quina:**
    *   **Ollama sobre RTX 3050 (Primary):** Generaci√≥n de tokens y comprensi√≥n de consultas.
    *   **OpenVINO sobre Intel NPU/iGPU (Background):** Pipeline independiente exclusivo para el procesamiento del *Retrieval* e inyecci√≥n (Embeddings vectoriales) y *Re-ranking*, utilizando librer√≠as nativas de aceleraci√≥n Intel.
2.  **Delegaci√≥n "Tool Calling" Estricta (Erradicaci√≥n de Alucinaci√≥n Matem√°tica):**
    *   Prohibir a los LLM (mediante prompts de sistema) realizar operaciones aritm√©ticas sobre cuotas o devoluciones del IVA/IRPF.
    *   Implementar rutinas puras en TypeScript que efect√∫en el c√°lculo, donde el LLM s√≥lo ejecuta un *"Llamado a Funci√≥n"* pasando los hiperpar√°metros detectados.
3.  **Hybrid Search Local Integrado con RRF (Reciprocal Rank Fusion):**
    *   Desplegar b√∫squedas complejas en la DB conectando `pgvector` (similitud sem√°ntica de dudas difusas) con *Full-Text Search BM25* (para t√©rminos exactos como "RD 1619/2012"). Combinar ambas puntuaciones mediante un motor RRF optimizado.

### üü† Prioridad 2: Soporte Estructural, Backend y Seguridad de Datos
*Transformar el prototipo de IA en una plataforma "Enterprise-Grade" resistente e inmutable.*

1.  **Row Level Security (RLS) Mandatorio en Todo el Scope:**
    *   Toda tabla creada en Supabase (`chats`, `labor_risk_assessments`, `invoices`, `rag_history`) debe requerir el `auth.uid()` del JWT. La pol√≠tica de seguridad a√≠sla herm√©ticamente los datos financieros y jur√≠dicos (Multitenancy puro).
2.  **Sistema de Gobernanza RBAC (Role-Based Access Control):**
    *   Extender el sistema de autenticaci√≥n para admitir campos escalares en perfiles `role = admin | partner | user`. Rutas y endpoints que afecten a la memoria del RAG, reingestas o parametrizaciones globales deben quedar fortificadas, validadas a del nivel SSR en Next.js.
3.  **Circuit Breakers y Rate Limiting Local:**
    *   Limitar las colas de peticiones. Debido a que el LLM se ejecuta local, el backend debe poner en 'Hold' o rechazar peticiones abusivas que puedan reventar el pool t√©rmico de la terminal o encolar la memoria.

### üü° Prioridad 3: UX y Features de Negocio de la "Advisor AI"
*Las interfaces de cara al cliente y asistentes integrados de productividad.*

1.  **Citas Bibliogr√°ficas Predictivas y Visuales (UI Contextual):**
    *   Garantizar que en la interfaz, el Assistant Chat retorne siempre "Citas accionables". Al pasar el cursor por un bloque citado de una respuesta de riesgo, renderizar *Popovers* que expongan exactamente el extracto legal de los cuadernos originales consumidos en la inyecci√≥n (Cumpliendo Feature `ANCLORA-CHAT-002`).
2.  **Predictividad en Facturaci√≥n e IRPF (Autofill AI):**
    *   Al intentar crear facturas o rellenar el Panel Fiscal, la inteligencia debe ser subrepticia (por detr√°s). Un modelo min√∫sculo observando variables `onBlur` que sugiera autocompletados (ej. aplicar exenciones formativas autom√°ticamente) para agilizar operativas.
3.  **Dashboard de Riesgos y Alertas Activas:**
    *   Cambiar de "Modo Conversaci√≥n Recreativo" a "Modo Asesor Proactivo". Cronjobs internos comparan las fechas de entregas impositivas vs. perfiles de usuario, incrustando Cards de acci√≥n de emergencia en el Home del Dashboard directamente de la base central.

### üü¢ Prioridad 4: Observabilidad, DevOps y Experiencia de Desarrollo
*Asegurar que se puede hacer crecer y depurar los fallos.*

1.  **Streaming Nativo Completo (UI Din√°mica):**
    *   El usuario jam√°s espera pantallas de "Cargando...". Implementaci√≥n directa del **Vercel AI SDK** en conjunto con la API para mandar fragmentos SSE (*Server Sent Events*), imprimiendo la asesor√≠a en tiempo real.
2.  **Adoptar Observabilidad RAG Open Source Centralizada:**
    *   Integrar un contenedor de [Langfuse](https://langfuse.com) y trazar todas las ejecuciones internamente. Podr√°s ver tiempos de latencia milisegundo a milisegundo, estimaciones de "costo simb√≥lico", rastrear tokens consumidos por iteraci√≥n y capturar likes/dislikes del cliente nativamente, todo 100% privado en tu ordenador.
3.  **Testing Suites "Smoke & Contracts":**
    *   Introducir robustez v√≠a Playwright / Vitest para testear E2E que las calculadoras puras funcionen, que el SSR rechace a no autorizados y que la firma de datos de las interacciones JSON con la IA siempre siga el contrato. Un c√≥digo sin regresiones.
